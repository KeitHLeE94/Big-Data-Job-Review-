{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logstash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 들어오는 데이터를 변형하거나 추가하는 기능.\n",
    "- 다양한 input, output pipeline 제공.\n",
    "- 수집된 데이터는 elastic search로 들어감.\n",
    "- hadoop eco system 데이터 수집 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 흐름을 위한 오픈소스 중앙 처리 엔진.(Beats와 함께 사용 권장)\n",
    "- dataflow 파이프라인을 구축하여 이벤트 데이터 변환 및 스트림 설정.\n",
    "    * 작게 시작하여 파이프라인을 늘려가는 방식 권장.\n",
    "    * 작은 데이터를 색인해보며 확장.\n",
    "- 다양한 데이터 자원 접근.\n",
    "- 원시 버퍼링을 통한 수평적 스케일링\n",
    "- 통합 처리를 위한 탄탄한 플러그인 환경."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 강화\n",
    "- 데이터스트림 실시간 변형, 제거, 정규화\n",
    "- 조건을 활용한 데이터 흐름 분리\n",
    "- 파이프라인 활성 상태 모니터링\n",
    "- SSL/TLS를 활용한 파이프라인 보안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동작 순서\n",
    "1. input: 데이터 수집 source에 대한 정의.\n",
    "    - 파일, syslog, sql query, http request, elastic search, beats, ...\n",
    "2. filter: 들어온 데이터를 어떻게 가공할지에 대한 정의.\n",
    "    - log 파싱, 데이터 추가, 태그 추가, ...\n",
    "3. output: 데이터를 어디로 보낼 것인지에 대한 정의.\n",
    "    - elastic search, 데이터 보관소, monitoring system, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: 데이터 처리를 위한 Logstash 설정\n",
    "- Codec: 데이터 인코딩, 디코딩을 위한 명세.\n",
    "    * json, avro, msgpack, netflow, cloudtrail, text, ...\n",
    "    * 원하는 모양으로 가공하여 전송 가능.\n",
    "- Work queue: Logstash의 filter가 작동하는 부분.\n",
    "    * worker를 늘려 성능 향상 가능.\n",
    "- Batcher: elastic search에 데이터를 보내기 위해 기본적으로 batch를 만들어 전송.\n",
    "    * 항상 Bulk api를 사용해야 좋은 성능.\n",
    "    * batch => bulk api로 전송하는 순서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logstash.yml\n",
    "- node.name: node 이름 지정 가능.\n",
    "- pipeline.workers: 더 많은 cpu core를 사용하고자 하면 늘리면 된다.\n",
    "- pipeline.output.workers: output별로 작업을 담당할 worker 수 지정 가능.\n",
    "- pipeline.batch.size: 들어오는 event data(log 한 라인)를 몇개씩 묶어 batch를 만들 것인지 지정.\n",
    "- pipeline.batch.delay: 몇초마다 원하는 목적지로 데이터를 보낼 것인지 지정.\n",
    " \n",
    " => batch.size는 늘리고 batch.delay는 줄이면 보다 많은 작업 실행 가능(heap memory 설정도 병행해야함)\n",
    " \n",
    "- Logstash: 실행 시 configuration 설정해준 다음 실행해줘야 함.(input, output 정의)\n",
    "- config.reload.automatic: true로 설정시 config 파일 내용 변경을 자동으로 반영.\n",
    "- config.reload.delay: 몇초마다 변경을 감지할 것인지.\n",
    "- path.config: 어떤 경로에서 config 파일을 가져올것인지 지정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline.yml\n",
    "- pipeline을 어디서 읽어올지 지정 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logstash 실행\n",
    "- bin/logstash -e 'input { stdin { } } output { stdout { } }': input = standard input, output = standard output으로 설정하여 실행.\n",
    "- bin/logstash -f simple.config: 미리 생성한 simple.config의 설정 내용을 반영하여 Logstash 실행.\n",
    "    * rubydebug 형식: version, host, timestamp를 자동으로 입력, 입력받은 모든 데이터는 message field 내에 입력됨.\n",
    "    * input codec 따로 지정 안하면 입력한 내용 그대로 들어감.(codec 지정시 지정된 codec 형식에 맞춰 들어감)\n",
    "    \n",
    "\n",
    "    input{\n",
    "     tcp{\n",
    "      port => 9900\n",
    "     }\n",
    "    }\n",
    "\n",
    "    filter{\n",
    "     grok{\n",
    "      match => { \"message\" => \"Hello %{WORD:name}\" }\n",
    "     }\n",
    "    }\n",
    "\n",
    "    output{\n",
    "     stdout{\n",
    "      codec => rubydebug\n",
    "     }\n",
    "    }\n",
    "- 입력: tcp로 받음\n",
    "- grok filter: message 내용을 name이라는 field에 저장.\n",
    "- output: stdout\n",
    "\n",
    "\n",
    "    입력: echo 'Hello Jaehyun' | nc localhost 990\n",
    "    \n",
    "    결과: \n",
    "    {\n",
    "      \"@version\" => \"1\",\n",
    "       \"message\" => \"Hello Jaehyun\",\n",
    "          \"name\" => \"Jaehyun\",\n",
    "          \"port\" => 61177,\n",
    "    \"@timestamp\" => 2018-08-12T17:10:37.460Z,\n",
    "          \"host\" => \"localhost\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    input{\n",
    "     tcp{\n",
    "      port => 9900\n",
    "      type => \"apache\"\n",
    "     }\n",
    "    }\n",
    "\n",
    "    filter{\n",
    "     if [type] == \"apache\"{\n",
    "\n",
    "      grok{\n",
    "       match => {\n",
    "        message => \"%{COMBINEDAPACHELOG}\"\n",
    "        remove_field => \"message\"\n",
    "       }\n",
    "      }\n",
    "\n",
    "      geoip{\n",
    "       source => \"clientip\"\n",
    "       fields => [\"city_name\", \"country_name\", \"location\", \"region_name\"]\n",
    "      }\n",
    "\n",
    "      date{\n",
    "       match => [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n",
    "       remove_field => \"timestamp\"\n",
    "      }\n",
    "     }\n",
    "    }\n",
    "\n",
    "    output{\n",
    "     elasticsearch{\n",
    "      hosts => [\"localhost:9200\"]\n",
    "     }\n",
    "    }\n",
    "- input: tcp에 apache type 추가\n",
    "- filter\n",
    "    * COMBINEDAPACHELOG: 로그 파싱을 위해 미리 정의되어 있음.\n",
    "    * geoip: ip주소를 통해 사용자의 지도상 위치를 가져올 수 있음.\n",
    "    * date: remove_field 사용 이유 - Logstash가 기본적으로 찍는 timestamp를 우리가 정의한 timestamp로 대체하기 위해 사용.\n",
    "- logstash가 알아서 config 바뀐 것을 감지하고 재로딩.\n",
    "\n",
    "\n",
    "    입력: 14.49.42.25 - - [10/Nov/2017:01:24:06 +0000] \"GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1\" 200 18848 \"-\" \"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.2b1) Gecko/20091014 Firefox/3.6b1 GTB5\"\n",
    "    결과: elastic search에 log 내용 입력됨.\n",
    "    \n",
    "    \n",
    "curl localhost:9600/_node?pretty: logstash 정보 가져올 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용 사례\n",
    "- twitter로 input 받을 수 있음. => twitter 내용을 elastic search로 실시간 색인 가능.\n",
    "- beats로 input: 출력을 logstash로 하여 필터링 후 output으로 elastic search.\n",
    "- output을 여러개 둘 수 있음: 동시에 output 전송 가능.\n",
    "- jdbc plugin 사용 => sql에서 가져올 수 있음.\n",
    "    * query를 통해 가져올 데이터 범위 지정 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
